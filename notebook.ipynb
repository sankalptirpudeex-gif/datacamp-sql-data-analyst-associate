{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Exam: Grocery Store Sales Analysis\n",
    "\n",
    "This notebook performs a complete end-to-end analysis for a grocery store chain (FoodYum). It covers data loading, cleaning, exploratory data analysis, feature engineering, simple modeling (monthly forecasting with linear regression), and basket pair analysis. The notebook is written to be robust: if a real CSV file is available it will be loaded, otherwise synthetic data will be generated so the analyses still run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data (or generate synthetic dataset if not present)\n",
    "\n",
    "The code tries to load a CSV from a few common paths. If not found, it generates a synthetic dataset with realistic columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_rows=20000, start_date='2023-01-01', end_date='2024-12-31'):\n",
    "    start = datetime.fromisoformat(start_date)\n",
    "    end = datetime.fromisoformat(end_date)\n",
    "    days = (end - start).days + 1\n",
    "    dates = [start + timedelta(days=int(x)) for x in np.random.randint(0, days, size=n_rows)]\n",
    "\n",
    "    store_ids = [f\"S{str(i).zfill(3)}\" for i in range(1, 11)]\n",
    "    product_catalog = []\n",
    "    categories = ['Produce', 'Meat', 'Dairy', 'Bakery', 'Beverages', 'Frozen', 'Household']\n",
    "    for pid in range(1, 301):\n",
    "        cat = np.random.choice(categories, p=[0.18, 0.12, 0.15, 0.12, 0.2, 0.12, 0.11])\n",
    "        name = f\"{cat[:3].upper()}_PROD_{pid}\"\n",
    "        price = round(abs(np.random.normal(loc=5 + categories.index(cat) * 1.5, scale=3)), 2) + 0.5\n",
    "        product_catalog.append((f\"P{str(pid).zfill(4)}\", name, cat, price))\n",
    "\n",
    "    product_df = pd.DataFrame(product_catalog, columns=['product_id', 'product_name', 'category', 'base_price'])\n",
    "\n",
    "    data = []\n",
    "    for i in range(n_rows):\n",
    "        pid, pname, cat, base_price = product_catalog[np.random.randint(0, len(product_catalog))]\n",
    "        qty = int(np.random.choice([1,1,1,2,2,3,4]))\n",
    "        price = round(base_price * (1 + np.random.normal(0, 0.05)), 2)\n",
    "        total = round(price * qty, 2)\n",
    "        s = np.random.choice(store_ids)\n",
    "        payment = np.random.choice(['cash', 'credit_card', 'debit_card', 'mobile_payment'], p=[0.15,0.5,0.25,0.1])\n",
    "        cust_id = f\"C{np.random.randint(1,2000):05d}\"\n",
    "        data.append((f'TX{str(i+1).zfill(7)}', dates[i], s, pid, pname, cat, qty, price, total, payment, cust_id))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['transaction_id', 'date', 'store_id', 'product_id', 'product_name', 'category', 'quantity', 'price', 'total_sale', 'payment_type', 'customer_id'])\n",
    "    # add store location mapping\n",
    "    store_loc = {f\"S{str(i).zfill(3)}\": np.random.choice(['North','South','East','West']) for i in range(1,11)}\n",
    "    df['store_region'] = df['store_id'].map(store_loc)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n",
    "\n",
    "possible_paths = [\n",
    "    'grocery_sales.csv',\n",
    "    'data/grocery_sales.csv',\n",
    "    'sales.csv',\n",
    "    'data/sales.csv'\n",
    "]\n",
    "df = None\n",
    "for p in possible_paths:\n",
    "    if os.path.exists(p):\n",
    "        try:\n",
    "            df = pd.read_csv(p, parse_dates=['date'])\n",
    "            print(f'Loaded data from {p}')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f'Failed to read {p}: {e}')\n",
    "\n",
    "if df is None:\n",
    "    print('No data file found. Generating synthetic dataset...')\n",
    "    df = generate_synthetic_data(n_rows=20000)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick data checks and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print('Rows, Columns:', df.shape)\n",
    "print('\\nMissing values per column:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Ensure types\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce').fillna(1).astype(int)\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce').fillna(0.0)\n",
    "df['total_sale'] = pd.to_numeric(df['total_sale'], errors='coerce').fillna(df['price'] * df['quantity'])\n",
    "\n",
    "# Drop rows with essential missing values\n",
    "essential = ['transaction_id','date','store_id','product_id']\n",
    "df = df.dropna(subset=essential)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature engineering\n",
    "\n",
    "Add month, day, weekday and revenue per transaction if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['weekday'] = df['date'].dt.day_name()\n",
    "df['month_start'] = df['date'].dt.to_period('M').dt.to_timestamp()\n",
    "df['revenue'] = df['total_sale']\n",
    "\n",
    "df[['date','transaction_id','store_id','product_id','category','quantity','price','total_sale']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "We'll look at top categories, monthly revenue trend, store performance, and weekday patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top categories by revenue\n",
    "cat_rev = df.groupby('category')['revenue'].sum().sort_values(ascending=False).reset_index()\n",
    "cat_rev['pct'] = 100 * cat_rev['revenue'] / cat_rev['revenue'].sum()\n",
    "cat_rev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot category revenue\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=cat_rev, x='revenue', y='category', palette='viridis')\n",
    "plt.title('Revenue by Category')\n",
    "plt.xlabel('Revenue')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly revenue trend\n",
    "monthly = df.groupby('month_start')['revenue'].sum().reset_index()\n",
    "monthly = monthly.sort_values('month_start')\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.lineplot(data=monthly, x='month_start', y='revenue', marker='o')\n",
    "plt.title('Monthly Revenue Trend')\n",
    "plt.ylabel('Revenue')\n",
    "plt.xlabel('Month')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store performance\n",
    "store_perf = df.groupby('store_id')['revenue'].sum().sort_values(ascending=False).reset_index()\n",
    "store_perf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(data=store_perf, x='store_id', y='revenue', palette='magma')\n",
    "plt.title('Revenue by Store')\n",
    "plt.xlabel('Store')\n",
    "plt.ylabel('Revenue')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Top products and baskets\n",
    "\n",
    "Identify the top selling products and common product pairs (simple co-occurrence analysis at transaction level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top products by revenue and quantity\n",
    "top_prod_rev = df.groupby(['product_id','product_name'])['revenue'].sum().sort_values(ascending=False).reset_index().head(20)\n",
    "top_prod_qty = df.groupby(['product_id','product_name'])['quantity'].sum().sort_values(ascending=False).reset_index().head(20)\n",
    "top_prod_rev.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple frequent pairs: for each transaction consider all products in that transaction and count pairs\n",
    "from itertools import combinations\n",
    "\n",
    "tx_products = df.groupby('transaction_id')['product_id'].apply(lambda x: list(set(x))).reset_index()\n",
    "\n",
    "pair_counts = {}\n",
    "for products in tx_products['product_id']:\n",
    "    if len(products) < 2:\n",
    "        continue\n",
    "    for a, b in combinations(sorted(products), 2):\n",
    "        pair_counts[(a,b)] = pair_counts.get((a,b), 0) + 1\n",
    "\n",
    "pair_df = pd.DataFrame([{'product_a': a, 'product_b': b, 'count': c} for (a,b),c in pair_counts.items()])\n",
    "pair_df = pair_df.sort_values('count', ascending=False).head(20)\n",
    "pair_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Simple forecasting: predict next month's revenue using linear regression\n",
    "\n",
    "This is a toy example that uses the monthly aggregated revenue and fits a linear model on time index to demonstrate basic forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly = df.groupby('month_start')['revenue'].sum().reset_index().sort_values('month_start')\n",
    "monthly['month_index'] = (monthly['month_start'] - monthly['month_start'].min()).dt.days // 30\n",
    "\n",
    "X = monthly[['month_index']].values\n",
    "y = monthly['revenue'].values\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "print('Train R2:', r2_score(y, y_pred))\n",
    "print('Train RMSE:', np.sqrt(mean_squared_error(y, y_pred)))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(monthly['month_start'], y, label='Actual', marker='o')\n",
    "plt.plot(monthly['month_start'], y_pred, label='Linear forecast', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Monthly revenue and linear fit')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Forecast next 3 months\n",
    "last_index = monthly['month_index'].max()\n",
    "future_idx = np.array([[last_index + i] for i in range(1,4)])\n",
    "future_pred = model.predict(future_idx)\n",
    "future_dates = [monthly['month_start'].max() + pd.DateOffset(months=i) for i in range(1,4)]\n",
    "pd.DataFrame({'month_start': future_dates, 'predicted_revenue': future_pred})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Recommendations and next steps\n",
    "\n",
    "1. Investigate promotions / holidays: add a promotions and holiday flag to the dataset to capture spikes.\n",
    "2. Use a time-series model (Prophet, SARIMA) for accurate forecasting and to capture seasonality.\n",
    "3. Use association-rule mining (apriori) for richer basket insights and build targeted cross-sell campaigns.\n",
    "4. Build per-store dashboards to monitor KPIs (revenue, avg basket, conversion) and set alerts for anomalies.\n",
    "\n",
    "This notebook is intentionally self-contained: it will generate data if none is found so you can run and inspect the analysis immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Save cleaned data (optional)\n",
    "Uncomment and provide a path if you'd like to persist the cleaned and enriched dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save\n",
    "# df.to_csv('cleaned_grocery_sales.csv', index=False)\n",
    "# print('Saved cleaned_grocery_sales.csv')\n",
    "print('Notebook finished.')\n"
   ]
  }
 ]
}
